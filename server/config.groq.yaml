# ["meta-llama/llama-4-maverick-17b-128e-instruct", "mixtral-8x7b-32768", "llama3-8b-8192", "llama3-70b-8192", "deepseek-coder", "deepseek-coder-33b", "gemma-7b-it"]
llms:
  information_supervisor:
    provider: groq
    model: meta-llama/llama-4-maverick-17b-128e-instruct #deepseek-r1-distill-llama-70b 
  source_code:
    provider: groq
    model: meta-llama/llama-4-maverick-17b-128e-instruct # deepseek-r1-distill-llama-70b  # Options: 
    #temperature: 0.7
  information_git:
    provider: groq
    model: meta-llama/llama-4-maverick-17b-128e-instruct # deepseek-r1-distill-llama-70b 
  information_github:
    provider: groq
    model: meta-llama/llama-4-maverick-17b-128e-instruct # deepseek-r1-distill-llama-70b 
  information_docs:
    provider: groq
    model: meta-llama/llama-4-maverick-17b-128e-instruct # deepseek-r1-distill-llama-70b 
  response_supervisor:
    provider: groq
    model: meta-llama/llama-4-maverick-17b-128e-instruct # deepseek-r1-distill-llama-70b 
  response_chat:
    provider: groq
    model: llama-3.3-70b-versatile
  diagram_painter:
    provider: anthropic
    model: claude-3-7-sonnet-20250219 #meta-llama/llama-4-maverick-17b-128e-instruct # deepseek-r1-distill-llama-70b
datasource:
  database:
    